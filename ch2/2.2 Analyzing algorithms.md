## 2.2 分析算法

&emsp;***分析（Analyzing）***一个算法意味着需要预测算法必要的资源。有时，主要关注内存、通信带宽或者计算机硬件资源，但是大多数情况下，只关注计算时间。一般来说，通过分析一个问题的几种候选算法，我们可以确定一个最有效的算法。这样的分析可能表明不止一个可行的候选算法，但是我们通常可以在这个过程中丢弃几个较差的算法。

&emsp;在分析算法之前，我们必须有一个将使用的实现技术的模型，包括该技术的资源及其成本的模型。对于本书的大部分内容，我们将假设一个通用的单处理器、***随机存取机（RAM，random-access machine）***的计算模型作为我们的实现技术，并注意算法将作为计算机程序实现。在RAM模型中，指令一个接一个地执行，没有并发操作。

&emsp;严格地说，我们应该精确地定义RAM模型的指令及其成本。然而，这样做将是单调乏味的，而且对算法设计和分析也没有什么深入的了解。然而，我们必须小心，不要滥用RAM模型。例如，如果RAM有一个排序的指令怎么办？如果是，我们就可以用一条指令完成排序。这样的RAM是不现实的，因为真正的计算机没有这样的指令。因此，我们需要以真正的计算机作为参考，以此设计算法。RAM模型包含在实际计算机中常见的指令：算术（如加法、减法、乘法、除法、余数、下限、上限）、数据移动（加载、存储、复制）和控制（条件和无条件分支、子例程调用和返回）。每一条这样的指令都需要确定长度的时间。

&emsp;RAM模型中的数据类型是整数和浮点数(用于存储实数)。虽然在本书中我们通常不关心精度，但在某些应用程序中，精度是至关重要的。我们还假定数据的每个字的大小是有限制的。例如，当处理大小为n的输入时，我们通常假设整数由某个 常数 $c>1$ 所表示的 $c\lg{n}$ 位表示。我们需要假设 $c ≥ 1$ ，这样每个字都可以代表 n 中的值，以便我们能够索引这些独立的输入元素，并且我们限制 $c$ 是一个常量，这样避免每个字的大小任意大。

&emsp;真实的计算机包含上面没有列出的指令，这些指令在RAM模型中表示一个灰色区域。例如，取幂是一个时间常量指令吗？一般情况下，不是；当x和y是实数时，计算 $x^y$ 需要几个指令。然而，在有限的情况下，取幂是一个常量时间的操作。许多计算机都有一个“左移”指令，它在恒定的时间内将整数的位向左移动k个位置。在大多数计算机中，将一个整数的位向左移动一个位置相当于乘以2，因此将位向左移动k个位置相当于乘以 $2^k$。因此，只要k不超过一个计算机字中的比特数，这样的计算机就可以通过将整数1向左移动k个位置，在一个恒定时间指令中计算 $2^k$ 。我们将会尽量避免 ARM 模型中这样的灰色区域，但是当 k 是一个足够小的正整数的时候，我们将 $2^k$ 的计算看作是一个时间常量操作。

&emsp;在RAM模型中，我们不尝试对当前计算机中常见的内存层次结构建模。也就是说，我们不为缓存或虚拟内存建模。一些计算模型试图解释内存层次效应，这在实际机器上的实际程序中有时很重要。本书中的一些问题研究了内存层次效应，但在大多数情况下，本书中的分析不会考虑这些问题。包含内存层次结构的模型要比RAM模型复杂得多，因此很难使用它们。此外，ram模型分析通常是实际机器性能的优秀预测器。

&emsp;在RAM模型中分析一个简单的算法也是一个挑战。所需要的数学工具可能包括组合学、概率论、代数灵活性和识别公式中最重要的关系的能力。因为算法的行为对于每个可能的输入可能是不同的，所以我们需要一种方法来用简单、容易理解的公式来概括这种行为。

&emsp;尽管我们通常只选择一个机器模型来分析给定的算法，但是在决定如何表达我们的分析时，我们仍然面临许多选择。我们想要一种简单的编写和操作方法，显示算法的资源需求的重要特征，并抑制繁琐的细节。



#### 插入排序的分析

&emsp; INSERTION-SORT 程序所消耗的时间依赖于输入：排列一千个数字总比排列三个数字耗费的时间长。此外，消耗时间也和输入已有的有序程度相关。一般来说，算法所花费的时间随着输入的大小而增长，因此传统上将程序的运行时间描述为输入大小的函数。为此，我们需要更仔细地定义术语“运行时间”和“输入大小”。

&emsp;***输入大小（input size）***的最佳概念取决于所研究的问题。对于许多问题，例如排序或计算离散傅里叶变换，最自然的度量方法是输入项的数量——例如，用于排序的数组大小n。对于许多其他问题，例如两个整数相乘，输入大小的最佳度量是用普通二进制表示法表示输入所需的总比特数。有时候，用两个数字而不是一个数字来描述输入的大小更合适。例如，如果算法的输入是一个图，那么输入的大小可以用图中顶点和边的数量来描述。我们将指出我们研究的每个问题使用的输入大小度量。

&emsp;算法在特定输入上的***运行时间（running time）***是执行的基本操作或“步骤”的数量。定义步骤的概念很方便，因此它尽可能地与机器无关。就目前而言，让我们采取以下观点。执行伪代码的每一行都需要一定的时间。一行可能花费的时间与另一行不同，但是我们假设第 $i$ 行每次执行都花费 $c_i$ 时间，其中 $ci$ 是一个常量。这个观点与RAM模型是一致的，它还反映了伪代码将如何在大多数实际计算机上实现。

&emsp;在接下来的讨论中，INSERTIONSORT的运行时间表达式将从一个使用所有语句的 $c_i$ 常量的凌乱公式发展为一个更简洁、更容易操作的更简单的表示法。这种更简单的表示法还可以很容易地确定一种算法是否比另一种算法更有效。

&emsp;我们首先用每个语句的时间“成本”和执行每个语句的次数来表示插入排序过程。对于 $j=2, 3,  ...,n$ ，其中 $n=A.lengh$ ，我们用 $t_j$ 表示伪代码中第五行的第 $j$ 次 while 循环所消耗的时间。此外for循环和while循环的循环头通常会比循环体多运行一次。最后，我们假设注释不会占用运行时间。

![](https://bin-note-picture.oss-ap-southeast-1.aliyuncs.com/Introduction to Algorithms/ch2_pseudocode_2.2_1.png)

&emsp;算法的运行时间为每条执行语句的运行时间之和；步骤 $c_i$ 执行 $n$ 次语句，共耗费 $c_in$ 时间。假设 $T(n)$ 表示 伪代码 INSERTION-SORT 在输入个数为 $n$ 时的总运行时间，则其表达式为：

![](https://bin-note-picture.oss-ap-southeast-1.aliyuncs.com/Introduction to Algorithms/ch2_2.2_formula_1.png)



#### 练习

**2.2-1** 用 Θ 记号表示函数 $n^3/1000-100n^2-100n+3$。

> Θ（n^3）



**2.2-2** 
=======


#### 练习

***2.2-1*** 用 Θ 记号表示函数 $ n^2/1000-100n^2-100n+3$ 。

``` 
Θ(n^2)
```



***2.2-2*** 考虑排序存储在数组 A 中的 n 个数：首先找出 A 中的最小元素并将其与 A[1] 中的元素进行交换。接着，找出 A 中的次最小元素并将其与 A[2] 中的元素进行交换。对 A 中前 n-1 个元素按该方式继续。该算法称为 **选择算法**，写出其伪代码，该算法维持的循环不变式是什么？为什么它只需要对前 n-1 个元素，而不是对所有 n 个元素运行？用 Θ 记号给出选择排序的最好情况与最坏情况运行时间。

```

```

>>>>>>> Stashed changes
